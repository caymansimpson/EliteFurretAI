import argparse
import asyncio
import copy
import os
import queue
import threading
from typing import List

import torch
from poke_env.ps_client import AccountConfiguration, ServerConfiguration

from elitefurretai.etl.team_repo import TeamRepo
from elitefurretai.rl.agent import RNaDAgent
from elitefurretai.rl.learner import RNaDLearner
from elitefurretai.rl.train import collate_trajectories, load_model
from elitefurretai.rl.worker import BatchInferencePlayer


def worker_loop(
    model: RNaDAgent,
    victim_model: RNaDAgent,
    traj_queue: queue.Queue,
    num_players: int,
    worker_id: int,
    exploiter_team: str,
    victim_teams: List[str],
    batch_size: int,
    device: str,
):
    """
    Runs an asyncio loop for a set of players battling a victim model.

    Args:
        exploiter_team: Fixed team for the exploiter (PokePaste format)
        victim_teams: List of teams for victims (one per victim player, PokePaste format)
        batch_size: Batch size for inference
        device: Device to use (cuda or cpu)
    """
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)

    players = []
    victims = []

    for i in range(num_players):
        # Exploiter (uses same team for all exploiter players)
        player = BatchInferencePlayer(
            model=model,
            device=device,
            batch_size=batch_size,
            player_configuration=AccountConfiguration(f"Exploiter_{worker_id}_{i}", None),
            server_configuration=ServerConfiguration(
                "ws://localhost:8000/showdown/websocket",
                None,  # type: ignore[arg-type]
            ),
            trajectory_queue=traj_queue,
            team=exploiter_team,
        )
        players.append(player)

        # Victim (Frozen) - each gets a unique team
        victim = BatchInferencePlayer(
            model=victim_model,
            device=device,
            batch_size=batch_size,
            player_configuration=AccountConfiguration(f"Victim_{worker_id}_{i}", None),
            server_configuration=ServerConfiguration(
                "ws://localhost:8000/showdown/websocket",
                None,  # type: ignore[arg-type]
            ),
            probabilistic=True,
            team=victim_teams[i],  # Use unique team for each victim
        )
        victims.append(victim)

    # Start inference loops
    for p in players:
        loop.create_task(p.start_inference_loop())
    for v in victims:
        loop.create_task(v.start_inference_loop())

    async def run_battles():
        tasks = []
        for p, v in zip(players, victims):
            tasks.append(p.battle_against(v, n_battles=100000))
        await asyncio.gather(*tasks)

    try:
        loop.run_until_complete(run_battles())
    except Exception as e:
        print(f"Worker {worker_id} crashed: {e}")


def main():
    parser = argparse.ArgumentParser(
        description="Train an exploiter agent against a victim model"
    )
    parser.add_argument(
        "--victim", type=str, required=True, help="Path to victim model checkpoint"
    )
    parser.add_argument(
        "--team-pool",
        type=str,
        required=True,
        help="Path to team pool directory for the victim and exploiter",
    )
    parser.add_argument(
        "--steps", type=int, default=50000, help="Number of training updates"
    )
    parser.add_argument(
        "--eval-games", type=int, default=100, help="Number of evaluation games"
    )
    parser.add_argument(
        "--threshold", type=float, default=0.6, help="Win rate threshold to save exploiter"
    )
    parser.add_argument(
        "--output-dir",
        type=str,
        default="data/models/exploiters",
        help="Directory to save exploiter",
    )
    parser.add_argument(
        "--checkpoint",
        type=str,
        default="data/models/bc_action_model.pt",
        help="Base model checkpoint",
    )
    parser.add_argument(
        "--num-workers", type=int, default=4, help="Number of worker threads"
    )
    parser.add_argument(
        "--players-per-worker",
        type=int,
        default=4,
        help="Number of concurrent players per worker",
    )
    parser.add_argument(
        "--batch-size", type=int, default=32, help="Batch size for inference"
    )
    parser.add_argument(
        "--learning-rate", type=float, default=1e-4, help="Learning rate for optimizer"
    )
    parser.add_argument(
        "--ent-coef", type=float, default=0.02, help="Entropy coefficient for exploration"
    )
    parser.add_argument(
        "--train-batch-size", type=int, default=64, help="Batch size for training updates"
    )
    parser.add_argument(
        "--format", type=str, default="gen9vgc2023regc", help="Pokemon format"
    )
    parser.add_argument(
        "--device",
        type=str,
        default="cuda" if torch.cuda.is_available() else "cpu",
        help="Device to use (cuda or cpu)",
    )

    args = parser.parse_args()

    print("=" * 60)
    print("Initializing Exploiter Training...")
    print(f"Victim: {args.victim}")
    print(f"Training steps: {args.steps}")
    print(f"Eval games: {args.eval_games}")
    print(f"Win threshold: {args.threshold}")
    print("Workers: {args.num_workers}, Players per worker: {args.players_per_worker}")
    print(f"Batch size: {args.batch_size}, Train batch size: {args.train_batch_size}")
    print(f"Learning rate: {args.learning_rate}")
    print(f"Device: {args.device}")
    print("=" * 60)

    # Select a single team for exploiter training
    team_repo = TeamRepo(filepath=args.team_pool, verbose=False)
    total_victims = args.num_workers * args.players_per_worker

    # Sample one team for the exploiter
    exploiter_team = team_repo.sample_team(args.format)

    # Sample different teams for each victim
    victim_teams = team_repo.sample_n_teams(
        args.format, total_victims, with_replacement=False
    )

    # Load models
    base_model = load_model(args.checkpoint, args.device)
    agent = RNaDAgent(base_model)

    victim_base_model = load_model(args.victim, args.device)
    victim_agent = RNaDAgent(victim_base_model)
    # Freeze victim
    for param in victim_agent.parameters():
        param.requires_grad = False

    # Ref model is just a dummy here since alpha=0, but learner expects it
    ref_agent = RNaDAgent(copy.deepcopy(base_model))

    # No regularization since we're trying to exploit
    learner = RNaDLearner(
        agent,
        ref_agent,
        lr=args.learning_rate,
        rnad_alpha=0.0,
        device=args.device,
        ent_coef=args.ent_coef,
    )

    traj_queue: queue.Queue = queue.Queue()

    # Start workers (all using the same exploiter team, but different victim teams)
    threads: List[threading.Thread] = []
    for i in range(args.num_workers):
        # Each worker gets a slice of victim teams
        worker_victim_teams = victim_teams[
            i * args.players_per_worker : (i + 1) * args.players_per_worker
        ]
        t = threading.Thread(
            target=worker_loop,
            args=(
                agent,
                victim_agent,
                traj_queue,
                args.players_per_worker,
                i,
                exploiter_team,
                worker_victim_teams,
                args.batch_size,
                args.device,
            ),
        )
        t.daemon = True
        t.start()
        threads.append(t)

    print(f"Started {args.num_workers} workers.")

    # Training loop
    trajectories = []
    updates = 0
    max_updates = args.steps

    try:
        while updates < max_updates:
            try:
                traj = traj_queue.get(timeout=1.0)
                trajectories.append(traj)
            except queue.Empty:
                continue

            if len(trajectories) >= args.train_batch_size:
                batch = collate_trajectories(trajectories, args.device)
                metrics = learner.update(batch)
                updates += 1

                if updates % 100 == 0:
                    print(f"Update {updates}/{max_updates}: {metrics}")

                trajectories = []

        print("\n" + "=" * 60)
        print(f"Training complete! Reached {updates} updates.")
        print("=" * 60)

        # Save exploiter
        os.makedirs(args.output_dir, exist_ok=True)
        exploiter_path = os.path.join(
            args.output_dir, f"exploiter_vs_{os.path.basename(args.victim)}"
        )
        torch.save(agent.model.state_dict(), exploiter_path)
        print(f"Exploiter model saved to: {exploiter_path}")

        # Save team alongside exploiter
        if exploiter_team:
            team_path = exploiter_path.replace(".pt", "_team.txt")
            with open(team_path, "w") as f:
                f.write(exploiter_team)
            print(f"Exploiter team saved to: {team_path}")

        # Return both model path and team
        return {"model_path": exploiter_path, "team": exploiter_team}

    except KeyboardInterrupt:
        print("\nTraining interrupted by user.")
        return None


if __name__ == "__main__":
    main()
